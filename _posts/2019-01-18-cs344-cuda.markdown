---
layout: post
comments: true
title:  "Notes on CS344: Introduction to Parallel Programming"
excerpt: "Problem sets review. Some notes about parallel algorithms and CUDA."
date:   2019-01-18 14:00:00
mathjax: true
---

Overview of CUDA processing pipeline:
---
1.Allocate host/device buffers, fill device buffers with data to process.
{% highlight c++ %}
 int *h_buffer = new int[num_elements];
 memset(h_buffer, 0, num_elements * sizeof(int));
 
 int *d_buffer_in;
 cudaMalloc(&d_buffer_in, num_elements * sizeof(int));
 cudaMemcpy(d_buffer_in,   h_buffer,   num_elements * sizeof(int), cudaMemcpyHostToDevice);
 
 int *d_buffer_out;
 cudaMalloc(&d_buffer_out, num_elements * sizeof(int));
{% endhighlight %}  
2.Set parameters for kernel execution (grid size, thread block size, number of shared memory to allocate).
{% highlight c++ %}
 const dim3 blockSize(512, 1, 1); // specify 512 threads for a thread block 
 const dim3 gridSize(ceil(num_elements / blockSize.x), 1, 1); // will launch ceil(num_elements / blockSize.x) number of thread blocks
{% endhighlight %}  
3.Launch a kernel with prepared data.  
{% highlight c++ %}
kernel<<<gridSize, blockSize>>>(d_buffer_in, d_buffer_out, num_elements);
{% endhighlight %}  
If kernel uses Shared Memory, we can specify the amount of memory to allocate dynamically:  
{% highlight c++ %}
kernel<<<gridSize, blockSize, number_of_elements_in_shared_mem * sizeof(int)>>>(d_buffer_in, d_buffer_out, num_elements);
{% endhighlight %}  
4.Synchronize CUDA device before retreiving data from device buffers.  
{% highlight c++ %}
cudaDeviceSynchronize();
{% endhighlight %}   

**Problem set #1: Color to Greyscale Conversion**

RGB -> Grayscale conversion:
`gray = .299f * R + .587f * G + .114f * B`

Kernel implementation:
{% highlight c++ %}
__global__
void rgba_to_greyscale(const uchar4* const rgbaImage, unsigned char* const greyImage, int numRows, int numCols)
{
  //First create a mapping from the 2D block and grid locations
  //to an absolute 2D location in the image, then use that to
  //calculate a 1D offset
  uint tidx = blockIdx.x * blockDim.x + threadIdx.x;
  uint tidy = blockIdx.y * blockDim.y + threadIdx.y;

  uint pos_1d = numCols * tidy + tidx;

  if ( (tidx < numCols) && (tidy < numRows) )
  {
      float r = (float)rgbaImage[pos_1d].x;
      float g = (float)rgbaImage[pos_1d].y;
      float b = (float)rgbaImage[pos_1d].z;
      float channelSum = __fadd_rn(__fadd_rn(__fmul_rn(.299f , r) , __fmul_rn(.587f , g)) , __fmul_rn(.114f , b));

      greyImage[pos_1d] = static_cast<unsigned char>(channelSum);
  }
}
{% endhighlight %}

It's interesting to notice that (X * Y) + Z calculated on CPU doesn't equal (X * Y) + Z calculated on GPU, because GPU uses Fused Multiply-Add operation (FMA). So CPU version looks like `Round(Round(X * Y) + Z)` and GPU with FMA `Round(X * Y + Z)`. This moment may result in small difference between CPU and GPU output. In the kernel above I use explicit rounding to match the reference (CPU) result.    
**Problem set #2: Image Blurring**

Input image is in RGB format in which color components are interleaved (RGBRGBRGBRGB...). Working with such structure will result in ineficient memory access pattern.  
Steps to follow:  
1.Separate RGB channels that different color components are stored contiguously (RRRRR..., GGGGG..., BBBBB...).
{% highlight c++ %}
__global__
void separateChannels(const uchar4* const inputImageRGBA,
                      int numRows, int numCols,
                      unsigned char* const redChannel,
                      unsigned char* const greenChannel,
                      unsigned char* const blueChannel)
{
  uint idx = blockIdx.x * blockDim.x + threadIdx.x;
  uint idy = blockIdx.y * blockDim.y + threadIdx.y;
  
  if ( idx >= numCols || idy >= numRows )
  {
     return;
  }

  uchar4 pixel = inputImageRGBA[idy * numCols + idx];

  redChannel[idy * numCols + idx] = pixel.x;
  greenChannel[idy * numCols + idx] = pixel.y;
  blueChannel[idy * numCols + idx] = pixel.z;

}
{% endhighlight %}
Github <a href="https://github.com/mike9251/cs344/tree/master/Problem%20Sets/Problem%20Set%201">link</a>.<br><br>

2.Perform bluring on each color channel (convolve each channel with a filter).
{% highlight c++ %}
__global__
void gaussian_blur(const unsigned char* const inputChannel,
                   unsigned char* const outputChannel,
                   int numRows, int numCols,
                   const float* const filter, const int filterWidth)
{  
  // NOTE: If a thread's absolute position 2D position is within the image, but some of
  // its neighbors are outside the image, then you will need to be extra careful. Instead
  // of trying to read such a neighbor value from GPU memory (which won't work because
  // the value is out of bounds), you should explicitly clamp the neighbor values you read
  // to be within the bounds of the image. If this is not clear to you, then please refer
  // to sequential reference solution for the exact clamping semantics you should follow.

  uint idx = blockIdx.x * blockDim.x + threadIdx.x;
  uint idy = blockIdx.y * blockDim.y + threadIdx.y;
  
  if ( idx >= numCols || idy >= numRows )
  {
     return;
  }

  float sum = 0.0;
  for (int filter_y = -filterWidth/2; filter_y <= filterWidth/2; ++filter_y) {
      for (int filter_x = -filterWidth/2; filter_x <= filterWidth/2; ++filter_x) {
          int image_vert_clamp = MIN(MAX(filter_y + idy, 0), static_cast<int>(numRows - 1));
          int image_horiz_clamp = MIN(MAX(filter_x + idx, 0), static_cast<int>(numCols - 1));

          float img_value = static_cast<float>(inputChannel[image_vert_clamp * numCols + image_horiz_clamp]);
          float filter_value = filter[(filterWidth/2 + filter_y)*filterWidth + (filterWidth/2 + filter_x)];
          sum +=  img_value * filter_value; 
      }
  }

  outputChannel[idy * numCols + idx] = sum;
}
{% endhighlight %}
3.Combine blured R, G, B channels back to RGB.
{% highlight c++ %}
__global__
void recombineChannels(const unsigned char* const redChannel,
                       const unsigned char* const greenChannel,
                       const unsigned char* const blueChannel,
                       uchar4* const outputImageRGBA,
                       int numRows,
                       int numCols)
{
  const int2 thread_2D_pos = make_int2( blockIdx.x * blockDim.x + threadIdx.x,
                                        blockIdx.y * blockDim.y + threadIdx.y);

  const int thread_1D_pos = thread_2D_pos.y * numCols + thread_2D_pos.x;

  if (thread_2D_pos.x >= numCols || thread_2D_pos.y >= numRows)
  {
     return;
  }

  unsigned char red   = redChannel[thread_1D_pos];
  unsigned char green = greenChannel[thread_1D_pos];
  unsigned char blue  = blueChannel[thread_1D_pos];

  //Alpha should be 255 for no transparency
  uchar4 outputPixel = make_uchar4(red, green, blue, 255);

  outputImageRGBA[thread_1D_pos] = outputPixel;
}
{% endhighlight %}

Github <a href="https://github.com/mike9251/cs344/tree/master/Problem%20Sets/Problem%20Set%202">link</a>.<br><br>  
1.Loading data  
Loading data for training SVM classifier is implemented in `load_data.py` in `def load_data(bShuffle=False, cs='YCrCb')` function. It loads images both classes (vehicles, non-vehicles) and converts color space to `cs`. Also data augmentation is performed - each image is horizontally flipped, so the result data set is doubled. Finally the dataset gets shuffled and splitted into train/val sets (90%/10%).  
Datasets: <a href="https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/vehicles.zip">Vehicle</a> and <a href="https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/non-vehicles.zip">Non-vehicle</a>.  
<div class="imgcap">
 <img src="/assets/self-driving-cars/car-detector/data_example.PNG" width="480" alt="Combined Image" />
 <div class="thecap">Non-vehicle and Vehicle examples</div>
</div>

2.Descriptor  
*2.1 Histogtram*  
Calculate histograms for each channel of the crop image (Window) and concatenate them into one vector. Use 256 bins,
smaller number of bins affects SVM's classification.  
<div class="imgcap">
<img src="/assets/self-driving-cars/car-detector/hist.PNG" width="480" alt="Combined Image" />
<div class="thecap">Histogram for one vehicle example</div>
</div>  

*2.2 Spatial Binning*  
To add some usefull information to the descriptor resize the crop (Window) to `32x32` pixels and unravel it into a vector.  

*2.3 Historgam of Oriented Gradients (HOG)*  
First of all gradient of the image is calculated. After calculating the gradient magnitude and orientation, we divide our
image into `cells` and `blocks`.  
A `cel` is a rectangular region of pixels that belong to this cell. For example, if we had a 128 x 128 image and defined 
our `pixels_per_cell` as 4x4, we would thus have 32 x 32 = 1024 cells.  

<div class="imgcap">
<img src="/assets/self-driving-cars/car-detector/cell.PNG" width="480" alt="Combined Image" />
<div class="thecap">Pick a cell</div>
</div>
<div class="imgcap">
<img src="/assets/self-driving-cars/car-detector/hog.PNG" width="480" alt="Combined Image" />
<div class="thecap">Gradient of the cell (Gx, Gy), its magnitute and orientations</div>
</div>  

Then calculated histograms are normalized. For this step `cells` are grouped into `blocks`.	For each of the cells in the 
current block we concatenate their corresponding gradient histograms and perfomr either L1 or L2 normalization of the entire 
concatenated feature vector. Normalization of HOGs increases performance of the descriptor.  

Parameters of the HOG descriptor extractor:  
`orientations=9` - define the number of bins in the gradient histogram;  
`pixels_per_cell=(8, 8)` - form cells with 8x8 pixels. HOG is calculated for each cell;  
`cells_per_block=(2, 2)` - form blocks of cells, normalize each cell's HOG wrt the entire block;  
`blocktransform_sqrt=True` - perform normalization;  
`block_norm="L1"` -  perform L1 block normalization;  
`feature_vector=True` - return descriptor in vector form (False - roi shape).  

Instead of calculating HOG features for each Window in the image do it once for the entire image and then extract 
features that correspond to the current Sliding Window. This approach improves performance.  
HOG explanations <a href="https://gurus.pyimagesearch.com/lesson-sample-histogram-of-oriented-gradients-and-car-logo-recognition/">1</a> and <a href="https://www.learnopencv.com/histogram-of-oriented-gradients/">2</a>.  

3.Support Vector Machine (SVM)  
Training SVM procedure is implemented in `svm.py`. First we normalize the image descriptors wiht `StandardScaler` from `sklearn.preprocessing` package. Then train a classifier `LinearSVC` from `sklearn.svm`. If a validation set is passed to the `train_svm` function the trained model gets evaluated on val set. Finally the model and the feature scaler are saved as `pkl` files.  

4.Sliding window  
Perform Sliding Window technique to look at multiple parts of the image and predict weather there is a car present. Define window parameters (size, stride) in terms of `cells`. (Implementation <a href="https://www.pyimagesearch.com/2015/03/23/sliding-windows-for-object-detection-with-python-and-opencv/">details</a>)  

5.Non-Maximum Suppression (NMS)  
The algorithm reduces the number of predicted bounding boxes. Pick a box, calculate IoU for this box and the rest of the boxes, discard boxes with IoU > thresh. Repeat. (Great NMS tutorial is <a href="https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/">here</a>) As a result we have smaller amount of boxes which are stored in a queue to take into acount detections in the previous frames (3 frames).  
<div class="imgcap">
 <img src="/assets/self-driving-cars/car-detector/16boxes.PNG" width="480" alt="Combined Image" />
 <div class="thecap">Detections before NMS</div>
</div>
<div class="imgcap">
 <img src="/assets/self-driving-cars/car-detector/7boxes.PNG" width="480" alt="Combined Image" />
 <div class="thecap">Detections after NMS</div>
</div>  

6.Heat map  
Calculate heat map to combine the detected bounding boxes into several regions which will represent final detections. First we create matrix of zeros with shape as input image. Then we increase the pixel intensity level by 1 at areas corresponding to detected boxes. For example, a pixel at some position of the heat map has value 5. It means that 5 boxes overlap at this position. At the end we binarize the obtained heat map by comparing its values with a threshold. Threshold value allows to choose how many boxes to consider for final detection. With `scipy.ndimage.measurements.label` function we obtain the segmented heat map (groups of boxes combined into several rectangular areas) and the number of segments. Each segment has different value, we use this knowledge in `draw_segment` function.  
<div class="imgcap">
 <img src="/assets/self-driving-cars/car-detector/heat_map.PNG" width="480" alt="Combined Image" />
 <div class="thecap">Heat map for detections remained after NMS</div>
</div>  

In function `draw_segment` we interpret the heat map and draw the final detections.
<div class="imgcap">
 <img src="/assets/self-driving-cars/car-detector/result.PNG" width="480" alt="Combined Image" />
 <div class="thecap">Detections obtained from the heat map</div>
</div>  

Final video (Classical Approach):
<div class="imgcap">
<iframe width="560" height="315" src="https://www.youtube.com/embed/TrMAK6ckAqc?ecver=1" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>

**Deep Learning approach**  

Use object detectors based on deep neural nets. For this step I've implemented YOLOv3 detector in PyTorch using this great <a href="https://blog.paperspace.com/tag/series-yolo/">tutorial</a>.  
Code is available <a href="https://github.com/mike9251/yolov3">here</a>.  

Final video (YOLOv3):
<div class="imgcap">
<iframe width="560" height="315" src="https://www.youtube.com/embed/jo234ZH5_po?ecver=1" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>  

**Summary**

Classical approach allows to detect cars but it is too slow (too far from real time processing). YOLOv3 detector shows more accurate results and much higher processing frame rate (i5-7300 ~ 0.5fps, on Nvidia 920mx ~ 3fps).
